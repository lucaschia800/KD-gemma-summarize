[2025-05-11 18:40:45,792] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:10<00:32, 10.91s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:10<00:32, 10.95s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:10<00:32, 10.96s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:26<00:26, 13.49s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:26<00:26, 13.50s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:26<00:26, 13.49s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:42<00:14, 14.80s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:42<00:14, 14.80s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:42<00:14, 14.80s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:54<00:00, 13.80s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:54<00:00, 13.71s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:54<00:00, 13.81s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:54<00:00, 13.72s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:54<00:00, 13.81s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:54<00:00, 13.72s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:18<00:18, 18.21s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:18<00:18, 18.18s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:18<00:18, 18.16s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:19<00:00,  8.16s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:19<00:00,  9.66s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:19<00:00,  8.16s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:19<00:00,  9.67s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:19<00:00,  8.19s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:19<00:00,  9.69s/it]
[2025-05-11 18:42:12,055] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-11 18:42:12,069] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-11 18:42:12,070] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-11 18:42:13,246] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-05-11 18:42:13,246] [INFO] [comm.py:700:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-05-11 18:42:13,265] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-05-11 18:42:13,299] [INFO] [comm.py:669:init_distributed] cdb=None
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
/transformers/src/transformers/tokenization_utils_base.py:2804: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/transformers/src/transformers/tokenization_utils_base.py:2804: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
  0%|          | 0/28924 [00:00<?, ?it/s]/transformers/src/transformers/tokenization_utils_base.py:2804: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
[rank1]: Traceback (most recent call last):
[rank1]:   File "/gscratch/stf/lbc800/mistral-KD/train_scripts/KD_HF.py", line 109, in <module>
[rank1]:     trainer.train()
[rank1]:   File "/transformers/src/transformers/trainer.py", line 2239, in train
[rank1]:     return inner_training_loop(
[rank1]:   File "/transformers/src/transformers/trainer.py", line 2554, in _inner_training_loop
[rank1]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank1]:   File "/transformers/src/transformers/trainer.py", line 3743, in training_step
[rank1]:     loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
[rank1]: TypeError: KDTrainer.compute_loss() got an unexpected keyword argument 'num_items_in_batch'
[rank2]: Traceback (most recent call last):
[rank2]:   File "/gscratch/stf/lbc800/mistral-KD/train_scripts/KD_HF.py", line 109, in <module>
[rank2]:     trainer.train()
[rank2]:   File "/transformers/src/transformers/trainer.py", line 2239, in train
[rank2]:     return inner_training_loop(
[rank2]:   File "/transformers/src/transformers/trainer.py", line 2554, in _inner_training_loop
[rank2]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank2]:   File "/transformers/src/transformers/trainer.py", line 3743, in training_step
[rank2]:     loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
[rank2]: TypeError: KDTrainer.compute_loss() got an unexpected keyword argument 'num_items_in_batch'
[rank0]: Traceback (most recent call last):
[rank0]:   File "/gscratch/stf/lbc800/mistral-KD/train_scripts/KD_HF.py", line 109, in <module>
[rank0]:     trainer.train()
[rank0]:   File "/transformers/src/transformers/trainer.py", line 2239, in train
[rank0]:     return inner_training_loop(
[rank0]:   File "/transformers/src/transformers/trainer.py", line 2554, in _inner_training_loop
[rank0]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank0]:   File "/transformers/src/transformers/trainer.py", line 3743, in training_step
[rank0]:     loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
[rank0]: TypeError: KDTrainer.compute_loss() got an unexpected keyword argument 'num_items_in_batch'
[rank0]:[W511 18:42:26.228322147 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
  0%|          | 0/28924 [00:02<?, ?it/s]
W0511 18:42:27.842000 84315 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 90031 closing signal SIGTERM
W0511 18:42:27.842000 84315 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 90032 closing signal SIGTERM
E0511 18:42:27.956000 84315 torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 0 (pid: 90030) of binary: /usr/bin/python3
Traceback (most recent call last):
  File "/usr/local/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/usr/local/lib/python3.10/dist-packages/accelerate/commands/accelerate_cli.py", line 50, in main
    args.func(args)
  File "/usr/local/lib/python3.10/dist-packages/accelerate/commands/launch.py", line 1196, in launch_command
    deepspeed_launcher(args)
  File "/usr/local/lib/python3.10/dist-packages/accelerate/commands/launch.py", line 878, in deepspeed_launcher
    distrib_run.run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 909, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/gscratch/stf/lbc800/mistral-KD/train_scripts/KD_HF.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-05-11_18:42:27
  host      : g3102.hyak.local
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 90030)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
srun: error: g3102: task 0: Exited with exit code 1
