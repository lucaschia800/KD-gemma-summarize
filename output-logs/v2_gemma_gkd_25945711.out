[2025-05-08 02:19:28,695] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-08 02:19:36,036] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-08 02:19:36,146] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-08 02:19:36,246] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:21<01:03, 21.08s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:42<00:42, 21.47s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:05<00:21, 21.97s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:16<00:00, 17.79s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:16<00:00, 19.20s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:16<00:16, 16.63s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:17<00:00,  7.37s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:17<00:00,  8.76s/it]
[2025-05-08 02:21:16,945] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-05-08 02:21:16,946] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-05-08 02:21:16,947] [INFO] [comm.py:669:init_distributed] cdb=None
[rank0]: Traceback (most recent call last):
[rank0]:   File "/gscratch/stf/lbc800/mistral-KD/train_scripts/GKD.py", line 63, in <module>
[rank0]:     train_args = GKDConfig(
[rank0]:   File "<string>", line 155, in __init__
[rank0]:   File "/usr/local/lib/python3.10/dist-packages/trl/trainer/gkd_config.py", line 100, in __post_init__
[rank0]:     super().__post_init__()
[rank0]:   File "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_config.py", line 207, in __post_init__
[rank0]:     super().__post_init__()
[rank0]:   File "/transformers/src/transformers/training_args.py", line 2009, in __post_init__
[rank0]:     self.deepspeed_plugin = DeepSpeedPlugin(hf_ds_config=self.hf_deepspeed_config)
[rank0]:   File "<string>", line 17, in __init__
[rank0]:   File "/usr/local/lib/python3.10/dist-packages/accelerate/utils/dataclasses.py", line 1176, in __post_init__
[rank0]:     self._deepspeed_config_checks()
[rank0]:   File "/usr/local/lib/python3.10/dist-packages/accelerate/utils/dataclasses.py", line 1390, in _deepspeed_config_checks
[rank0]:     raise ValueError(
[rank0]: ValueError: When using `deepspeed_config_file`, the following accelerate config variables will be ignored: ['gradient_accumulation_steps', 'gradient_clipping', 'zero_stage', 'offload_optimizer_device', 'offload_param_device', 'offload_param_nvme_path', 'offload_optimizer_nvme_path', 'zero3_save_16bit_model', 'mixed_precision'].
[rank0]: Please specify them appropriately in the DeepSpeed config file.
[rank0]: If you are using an accelerate config file, remove others config variables mentioned in the above specified list.
[rank0]: The easiest method is to create a new config following the questionnaire via `accelerate config`.
[rank0]: It will only ask for the necessary config variables when using `deepspeed_config_file`.
[rank0]:[W508 02:21:18.990727659 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
W0508 02:21:21.467000 90464 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 90610 closing signal SIGTERM
W0508 02:21:21.467000 90464 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 90611 closing signal SIGTERM
E0508 02:21:21.613000 90464 torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 0 (pid: 90609) of binary: /usr/bin/python3
Traceback (most recent call last):
  File "/usr/local/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/usr/local/lib/python3.10/dist-packages/accelerate/commands/accelerate_cli.py", line 50, in main
    args.func(args)
  File "/usr/local/lib/python3.10/dist-packages/accelerate/commands/launch.py", line 1196, in launch_command
    deepspeed_launcher(args)
  File "/usr/local/lib/python3.10/dist-packages/accelerate/commands/launch.py", line 878, in deepspeed_launcher
    distrib_run.run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 909, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/gscratch/stf/lbc800/mistral-KD/train_scripts/GKD.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-05-08_02:21:21
  host      : g3109.hyak.local
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 90609)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
srun: error: g3109: task 0: Exited with exit code 1
