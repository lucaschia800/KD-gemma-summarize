[2025-05-13 15:16:50,731] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:10<00:30, 10.25s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:10<00:30, 10.21s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:10<00:30, 10.29s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:21<00:21, 10.67s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:21<00:21, 10.67s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:21<00:21, 10.66s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:31<00:10, 10.70s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:31<00:10, 10.70s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:31<00:10, 10.71s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:40<00:00,  9.67s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:40<00:00, 10.01s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:40<00:00,  9.69s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:40<00:00,  9.69s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:40<00:00, 10.02s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:40<00:00, 10.03s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:18<00:18, 18.74s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:18<00:18, 18.79s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:18<00:18, 18.86s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:20<00:00,  8.64s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:20<00:00, 10.17s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:20<00:00,  8.62s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:20<00:00, 10.14s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:20<00:00,  8.65s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:20<00:00, 10.17s/it]
[2025-05-13 15:18:06,101] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-13 15:18:06,113] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-13 15:18:06,114] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-13 15:18:07,318] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-05-13 15:18:07,318] [INFO] [comm.py:700:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-05-13 15:18:07,455] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-05-13 15:18:07,510] [INFO] [comm.py:669:init_distributed] cdb=None
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Parameter Offload: Total persistent parameters: 241920 in 105 params
  0%|          | 0/28924 [00:00<?, ?it/s]/transformers/src/transformers/tokenization_utils_base.py:2804: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/transformers/src/transformers/tokenization_utils_base.py:2804: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/transformers/src/transformers/tokenization_utils_base.py:2804: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
[rank1]: Traceback (most recent call last):
[rank1]:   File "/gscratch/stf/lbc800/mistral-KD/train_scripts/KD_HF.py", line 111, in <module>
[rank1]:     trainer.train()
[rank1]:   File "/transformers/src/transformers/trainer.py", line 2239, in train
[rank1]:     return inner_training_loop(
[rank1]:   File "/transformers/src/transformers/trainer.py", line 2554, in _inner_training_loop
[rank1]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank1]:   File "/transformers/src/transformers/trainer.py", line 3743, in training_step
[rank1]:     loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
[rank1]:   File "/gscratch/stf/lbc800/mistral-KD/train_scripts/KD_HF.py", line 46, in compute_loss
[rank1]:     kd_loss = nn.KLDivLoss(reduction="batchmean")(soft_student, soft_teacher) * (T**2)  #reverse KL
[rank1]:   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1750, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py", line 543, in forward
[rank1]:     return F.kl_div(
[rank1]:   File "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py", line 3396, in kl_div
[rank1]:     reduced = torch.kl_div(input, target, reduction_enum, log_target=log_target)
[rank1]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.95 GiB. GPU 1 has a total capacity of 44.42 GiB of which 1.92 GiB is free. Including non-PyTorch memory, this process has 42.49 GiB memory in use. Of the allocated memory 39.98 GiB is allocated by PyTorch, and 1.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
W0513 15:18:23.082000 45253 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 45855 closing signal SIGTERM
W0513 15:18:23.082000 45253 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 45857 closing signal SIGTERM
E0513 15:18:23.716000 45253 torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 1 (pid: 45856) of binary: /usr/bin/python3
Traceback (most recent call last):
  File "/usr/local/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/usr/local/lib/python3.10/dist-packages/accelerate/commands/accelerate_cli.py", line 50, in main
    args.func(args)
  File "/usr/local/lib/python3.10/dist-packages/accelerate/commands/launch.py", line 1196, in launch_command
    deepspeed_launcher(args)
  File "/usr/local/lib/python3.10/dist-packages/accelerate/commands/launch.py", line 878, in deepspeed_launcher
    distrib_run.run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 909, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/gscratch/stf/lbc800/mistral-KD/train_scripts/KD_HF.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-05-13_15:18:23
  host      : g3112.hyak.local
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 45856)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
srun: error: g3112: task 0: Exited with exit code 1
