[2025-05-11 23:19:08,436] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:16<00:50, 16.91s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:16<00:50, 16.91s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:16<00:50, 16.92s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:37<00:37, 18.97s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:37<00:37, 18.97s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:37<00:37, 18.99s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:58<00:20, 20.17s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:58<00:20, 20.17s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:58<00:20, 20.17s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:15<00:00, 18.94s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:15<00:00, 18.99s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:15<00:00, 18.95s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:15<00:00, 19.00s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:15<00:00, 18.95s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:15<00:00, 19.00s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:24<00:24, 24.39s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:24<00:24, 24.37s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:24<00:24, 24.40s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:25<00:00, 10.79s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:25<00:00, 12.82s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:25<00:00, 10.80s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:25<00:00, 12.84s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:25<00:00, 10.80s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:25<00:00, 12.84s/it]
[2025-05-11 23:21:05,280] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-11 23:21:05,280] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-11 23:21:05,291] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-11 23:21:06,470] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-05-11 23:21:06,470] [INFO] [comm.py:700:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-05-11 23:21:06,690] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-05-11 23:21:06,715] [INFO] [comm.py:669:init_distributed] cdb=None
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
/transformers/src/transformers/tokenization_utils_base.py:2804: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/transformers/src/transformers/tokenization_utils_base.py:2804: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
  0%|          | 0/28924 [00:00<?, ?it/s]/transformers/src/transformers/tokenization_utils_base.py:2804: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
[rank1]: Traceback (most recent call last):
[rank1]:   File "/gscratch/stf/lbc800/mistral-KD/train_scripts/KD_HF.py", line 109, in <module>
[rank1]:     trainer.train()
[rank1]:   File "/transformers/src/transformers/trainer.py", line 2239, in train
[rank1]:     return inner_training_loop(
[rank1]:   File "/transformers/src/transformers/trainer.py", line 2554, in _inner_training_loop
[rank1]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank1]:   File "/transformers/src/transformers/trainer.py", line 3743, in training_step
[rank1]:     loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
[rank1]:   File "/gscratch/stf/lbc800/mistral-KD/train_scripts/KD_HF.py", line 39, in compute_loss
[rank1]:     student_outputs = model(input_ids=input_ids, attention_mask=attention_mask).logits
[rank1]:   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1750, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:   File "/usr/local/lib/python3.10/dist-packages/deepspeed/utils/nvtx.py", line 20, in wrapped_fn
[rank1]:     ret_val = func(*args, **kwargs)
[rank1]:   File "/usr/local/lib/python3.10/dist-packages/deepspeed/runtime/engine.py", line 2054, in forward
[rank1]:     loss = self.module(*inputs, **kwargs)
[rank1]:   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank1]:     return inner()
[rank1]:   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1793, in inner
[rank1]:     result = forward_call(*args, **kwargs)
[rank1]:   File "/transformers/src/transformers/utils/generic.py", line 969, in wrapper
[rank1]:     output = func(self, *args, **kwargs)
[rank1]:   File "/transformers/src/transformers/models/gemma2/modeling_gemma2.py", line 856, in forward
[rank1]:     outputs: BaseModelOutputWithPast = self.model(
[rank1]:   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1750, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:   File "/transformers/src/transformers/utils/generic.py", line 969, in wrapper
[rank1]:     output = func(self, *args, **kwargs)
[rank1]:   File "/transformers/src/transformers/utils/deprecation.py", line 172, in wrapped_func
[rank1]:     return func(*args, **kwargs)
[rank1]:   File "/transformers/src/transformers/models/gemma2/modeling_gemma2.py", line 640, in forward
[rank1]:     layer_outputs = decoder_layer(
[rank1]:   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1750, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:   File "/transformers/src/transformers/utils/deprecation.py", line 172, in wrapped_func
[rank1]:     return func(*args, **kwargs)
[rank1]:   File "/transformers/src/transformers/models/gemma2/modeling_gemma2.py", line 345, in forward
[rank1]:     hidden_states = self.post_attention_layernorm(hidden_states)
[rank1]:   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1750, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:   File "/transformers/src/transformers/models/gemma2/modeling_gemma2.py", line 77, in forward
[rank1]:     output = self._norm(x.float())
[rank1]:   File "/transformers/src/transformers/models/gemma2/modeling_gemma2.py", line 74, in _norm
[rank1]:     return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
[rank1]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 36.00 MiB. GPU 1 has a total capacity of 44.42 GiB of which 10.12 MiB is free. Including non-PyTorch memory, this process has 44.40 GiB memory in use. Of the allocated memory 43.58 GiB is allocated by PyTorch, and 132.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank2]: Traceback (most recent call last):
[rank2]:   File "/gscratch/stf/lbc800/mistral-KD/train_scripts/KD_HF.py", line 109, in <module>
[rank2]:     trainer.train()
[rank2]:   File "/transformers/src/transformers/trainer.py", line 2239, in train
[rank2]:     return inner_training_loop(
[rank2]:   File "/transformers/src/transformers/trainer.py", line 2554, in _inner_training_loop
[rank2]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank2]:   File "/transformers/src/transformers/trainer.py", line 3743, in training_step
[rank2]:     loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
[rank2]:   File "/gscratch/stf/lbc800/mistral-KD/train_scripts/KD_HF.py", line 39, in compute_loss
[rank2]:     student_outputs = model(input_ids=input_ids, attention_mask=attention_mask).logits
[rank2]:   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1750, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]:   File "/usr/local/lib/python3.10/dist-packages/deepspeed/utils/nvtx.py", line 20, in wrapped_fn
[rank2]:     ret_val = func(*args, **kwargs)
[rank2]:   File "/usr/local/lib/python3.10/dist-packages/deepspeed/runtime/engine.py", line 2054, in forward
[rank2]:     loss = self.module(*inputs, **kwargs)
[rank2]:   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank2]:     return inner()
[rank2]:   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1793, in inner
[rank2]:     result = forward_call(*args, **kwargs)
[rank2]:   File "/transformers/src/transformers/utils/generic.py", line 969, in wrapper
[rank2]:     output = func(self, *args, **kwargs)
[rank2]:   File "/transformers/src/transformers/models/gemma2/modeling_gemma2.py", line 856, in forward
[rank2]:     outputs: BaseModelOutputWithPast = self.model(
[rank2]:   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1750, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]:   File "/transformers/src/transformers/utils/generic.py", line 969, in wrapper
[rank2]:     output = func(self, *args, **kwargs)
[rank2]:   File "/transformers/src/transformers/utils/deprecation.py", line 172, in wrapped_func
[rank2]:     return func(*args, **kwargs)
[rank2]:   File "/transformers/src/transformers/models/gemma2/modeling_gemma2.py", line 640, in forward
[rank2]:     layer_outputs = decoder_layer(
[rank2]:   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1750, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]:   File "/transformers/src/transformers/utils/deprecation.py", line 172, in wrapped_func
[rank2]:     return func(*args, **kwargs)
[rank2]:   File "/transformers/src/transformers/models/gemma2/modeling_gemma2.py", line 350, in forward
[rank2]:     hidden_states = self.mlp(hidden_states)
[rank2]:   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1750, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]:   File "/transformers/src/transformers/models/gemma2/modeling_gemma2.py", line 99, in forward
[rank2]:     down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
[rank2]:   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1750, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]:   File "/transformers/src/transformers/activations.py", line 37, in forward
[rank2]:     return nn.functional.gelu(input, approximate="tanh")
[rank2]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 72.00 MiB. GPU 2 has a total capacity of 44.42 GiB of which 60.12 MiB is free. Including non-PyTorch memory, this process has 44.35 GiB memory in use. Of the allocated memory 43.58 GiB is allocated by PyTorch, and 108.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
W0511 23:21:30.699000 27114 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 28241 closing signal SIGTERM
E0511 23:21:30.996000 27114 torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 1 (pid: 28242) of binary: /usr/bin/python3
Traceback (most recent call last):
  File "/usr/local/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/usr/local/lib/python3.10/dist-packages/accelerate/commands/accelerate_cli.py", line 50, in main
    args.func(args)
  File "/usr/local/lib/python3.10/dist-packages/accelerate/commands/launch.py", line 1196, in launch_command
    deepspeed_launcher(args)
  File "/usr/local/lib/python3.10/dist-packages/accelerate/commands/launch.py", line 878, in deepspeed_launcher
    distrib_run.run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 909, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/gscratch/stf/lbc800/mistral-KD/train_scripts/KD_HF.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2025-05-11_23:21:30
  host      : g3104.hyak.local
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 28243)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-05-11_23:21:30
  host      : g3104.hyak.local
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 28242)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
srun: error: g3104: task 0: Exited with exit code 1
