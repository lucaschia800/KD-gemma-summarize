[2025-05-12 19:07:45,580] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:15<00:45, 15.07s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:15<00:45, 15.08s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:15<00:45, 15.09s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:37<00:38, 19.15s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:37<00:38, 19.15s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:37<00:38, 19.17s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:56<00:19, 19.09s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:56<00:19, 19.09s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:56<00:19, 19.10s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:09<00:00, 16.93s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:09<00:00, 17.43s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:09<00:00, 16.95s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:09<00:00, 17.44s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:09<00:00, 16.94s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:09<00:00, 17.44s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:22<00:22, 22.33s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:22<00:22, 22.33s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:22<00:22, 22.36s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:23<00:00, 10.04s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:23<00:00, 11.88s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:23<00:00, 10.03s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:23<00:00, 11.88s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:23<00:00, 10.04s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:23<00:00, 11.88s/it]
[2025-05-12 19:09:34,486] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-12 19:09:34,487] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-12 19:09:34,497] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-12 19:09:39,070] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-05-12 19:09:39,070] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-05-12 19:09:39,070] [INFO] [comm.py:700:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-05-12 19:09:39,070] [INFO] [comm.py:669:init_distributed] cdb=None
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Parameter Offload: Total persistent parameters: 241920 in 105 params
  0%|          | 0/28924 [00:00<?, ?it/s]/transformers/src/transformers/tokenization_utils_base.py:2804: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/transformers/src/transformers/tokenization_utils_base.py:2804: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/transformers/src/transformers/tokenization_utils_base.py:2804: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
[rank1]: Traceback (most recent call last):
[rank1]:   File "/gscratch/stf/lbc800/mistral-KD/train_scripts/KD_HF.py", line 111, in <module>
[rank1]:     trainer.train()
[rank1]:   File "/transformers/src/transformers/trainer.py", line 2239, in train
[rank1]:     return inner_training_loop(
[rank1]:   File "/transformers/src/transformers/trainer.py", line 2554, in _inner_training_loop
[rank1]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank1]:   File "/transformers/src/transformers/trainer.py", line 3743, in training_step
[rank1]:     loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
[rank1]:   File "/gscratch/stf/lbc800/mistral-KD/train_scripts/KD_HF.py", line 46, in compute_loss
[rank1]:     kd_loss = nn.KLDivLoss(reduction="batchmean")(soft_student, soft_teacher) * (T**2)  #reverse KL
[rank1]:   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1750, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py", line 543, in forward
[rank1]:     return F.kl_div(
[rank1]:   File "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py", line 3396, in kl_div
[rank1]:     reduced = torch.kl_div(input, target, reduction_enum, log_target=log_target)
[rank1]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.95 GiB. GPU 1 has a total capacity of 44.42 GiB of which 1.92 GiB is free. Including non-PyTorch memory, this process has 42.49 GiB memory in use. Of the allocated memory 39.97 GiB is allocated by PyTorch, and 1.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
W0512 19:09:59.005000 22441 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 24197 closing signal SIGTERM
W0512 19:09:59.008000 22441 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 24199 closing signal SIGTERM
E0512 19:09:59.456000 22441 torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 1 (pid: 24198) of binary: /usr/bin/python3
Traceback (most recent call last):
  File "/usr/local/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/usr/local/lib/python3.10/dist-packages/accelerate/commands/accelerate_cli.py", line 50, in main
    args.func(args)
  File "/usr/local/lib/python3.10/dist-packages/accelerate/commands/launch.py", line 1196, in launch_command
    deepspeed_launcher(args)
  File "/usr/local/lib/python3.10/dist-packages/accelerate/commands/launch.py", line 878, in deepspeed_launcher
    distrib_run.run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 909, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/gscratch/stf/lbc800/mistral-KD/train_scripts/KD_HF.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-05-12_19:09:59
  host      : g3100.hyak.local
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 24198)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
srun: error: g3100: task 0: Exited with exit code 1
