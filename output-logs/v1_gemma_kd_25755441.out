[2025-05-05 00:05:47,957] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-05 00:06:01,709] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2025-05-05 00:06:01,709] [INFO] [runner.py:605:main] cmd = /usr/bin/python3 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None /gscratch/stf/lbc800/mistral-KD/KD_HF.py
[2025-05-05 00:06:02,953] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-05 00:06:08,021] [INFO] [launch.py:139:main] 0 NCCL_VERSION=2.17.1-1
[2025-05-05 00:06:08,021] [INFO] [launch.py:139:main] 0 NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.17.1-1+cuda12.1
[2025-05-05 00:06:08,021] [INFO] [launch.py:139:main] 0 NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev
[2025-05-05 00:06:08,021] [INFO] [launch.py:139:main] 0 NV_LIBNCCL_DEV_PACKAGE_VERSION=2.17.1-1
[2025-05-05 00:06:08,021] [INFO] [launch.py:139:main] 0 NV_LIBNCCL_PACKAGE=libnccl2=2.17.1-1+cuda12.1
[2025-05-05 00:06:08,021] [INFO] [launch.py:139:main] 0 NV_LIBNCCL_PACKAGE_NAME=libnccl2
[2025-05-05 00:06:08,021] [INFO] [launch.py:139:main] 0 NV_LIBNCCL_PACKAGE_VERSION=2.17.1-1
[2025-05-05 00:06:08,021] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2025-05-05 00:06:08,021] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=2, node_rank=0
[2025-05-05 00:06:08,021] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2025-05-05 00:06:08,021] [INFO] [launch.py:164:main] dist_world_size=2
[2025-05-05 00:06:08,021] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1
[2025-05-05 00:06:08,034] [INFO] [launch.py:256:main] process 56679 spawned with command: ['/usr/bin/python3', '-u', '/gscratch/stf/lbc800/mistral-KD/KD_HF.py', '--local_rank=0']
[2025-05-05 00:06:08,038] [INFO] [launch.py:256:main] process 56680 spawned with command: ['/usr/bin/python3', '-u', '/gscratch/stf/lbc800/mistral-KD/KD_HF.py', '--local_rank=1']
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:13<00:40, 13.38s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:33<00:34, 17.26s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:50<00:17, 17.29s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:05<00:00, 16.29s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:05<00:00, 16.36s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:15<00:15, 15.92s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:16<00:00,  7.17s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:16<00:00,  8.48s/it]
[2025-05-05 00:07:49,308] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/usr/bin/ld: cannot find -laio: No such file or directory
collect2: error: ld returned 1 exit status
/usr/bin/ld: cannot find -laio: No such file or directory
collect2: error: ld returned 1 exit status
[2025-05-05 00:07:51,066] [INFO] [comm.py:669:init_distributed] cdb=None
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[rank0]: Traceback (most recent call last):
[rank0]:   File "/gscratch/stf/lbc800/mistral-KD/KD_HF.py", line 99, in <module>
[rank0]:     trainer.train()
[rank0]:   File "/transformers/src/transformers/trainer.py", line 2239, in train
[rank0]:     return inner_training_loop(
[rank0]:   File "/transformers/src/transformers/trainer.py", line 2368, in _inner_training_loop
[rank0]:     model, self.optimizer = self.accelerator.prepare(self.model, self.optimizer)
[rank0]:   File "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py", line 1440, in prepare
[rank0]:     result = self._prepare_deepspeed(*args)
[rank0]:   File "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py", line 2033, in _prepare_deepspeed
[rank0]:     engine, optimizer, _, lr_scheduler = ds_initialize(**kwargs)
[rank0]:   File "/usr/local/lib/python3.10/dist-packages/deepspeed/__init__.py", line 193, in initialize
[rank0]:     engine = DeepSpeedEngine(args=args,
[rank0]:   File "/usr/local/lib/python3.10/dist-packages/deepspeed/runtime/engine.py", line 278, in __init__
[rank0]:     self._configure_distributed_model(model)
[rank0]:   File "/usr/local/lib/python3.10/dist-packages/deepspeed/runtime/engine.py", line 1307, in _configure_distributed_model
[rank0]:     self._broadcast_model()
[rank0]:   File "/usr/local/lib/python3.10/dist-packages/deepspeed/runtime/engine.py", line 1225, in _broadcast_model
[rank0]:     dist.broadcast(p.data, groups._get_broadcast_src_rank(), group=self.seq_data_parallel_group)
[rank0]:   File "/usr/local/lib/python3.10/dist-packages/deepspeed/comm/comm.py", line 117, in log_wrapper
[rank0]:     return func(*args, **kwargs)
[rank0]:   File "/usr/local/lib/python3.10/dist-packages/deepspeed/comm/comm.py", line 224, in broadcast
[rank0]:     return cdb.broadcast(tensor=tensor, src=src, group=group, async_op=async_op)
[rank0]:   File "/usr/local/lib/python3.10/dist-packages/deepspeed/comm/torch.py", line 206, in broadcast
[rank0]:     return torch.distributed.broadcast(tensor=tensor, src=src, group=group, async_op=async_op)
[rank0]:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
[rank0]:     return func(*args, **kwargs)
[rank0]:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 2726, in broadcast
[rank0]:     work = group.broadcast([tensor], opts)
[rank0]:   File "/usr/local/lib/python3.10/dist-packages/torch/_compile.py", line 32, in inner
[rank0]:     return disable_fn(*args, **kwargs)
[rank0]:   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py", line 745, in _fn
[rank0]:     return fn(*args, **kwargs)
[rank0]:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/tensor/_api.py", line 346, in __torch_dispatch__
[rank0]:     return DTensor._op_dispatcher.dispatch(
[rank0]:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/tensor/_dispatch.py", line 167, in dispatch
[rank0]:     op_info = self.unwrap_to_op_info(op_call, args, kwargs)
[rank0]:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/tensor/_dispatch.py", line 400, in unwrap_to_op_info
[rank0]:     assert mesh is not None, f"found no DeviceMesh from dtensor args for {op_call}!"
[rank0]: AssertionError: found no DeviceMesh from dtensor args for c10d.broadcast_.default!
[rank0]:[W505 00:07:56.768465161 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[2025-05-05 00:08:01,228] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 56679
[2025-05-05 00:08:01,252] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 56680
[2025-05-05 00:08:01,253] [ERROR] [launch.py:325:sigkill_handler] ['/usr/bin/python3', '-u', '/gscratch/stf/lbc800/mistral-KD/KD_HF.py', '--local_rank=1'] exits with return code = 1
